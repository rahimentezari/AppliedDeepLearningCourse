# AppliedDeepLearningCourse
This respository is built during my course in TU Wien in Winter 2019, Applied Deep Learning by Dr. Alexander Pacha.
### Topic : Neural Network Compression
#### Application : Computer Vision (medical imaging)
#### Project Type: Bring your own method
#### Introduction
Todayâ€™s deep neural networks require substantial computation resources for their training, storage and inference, which
limits their effective use on resource-constrained devices. Recently there has been more focus on neural network compression. This includes different approaches, i.e. Quantization and binarization, Decomposition and factorization, Knowledge distillation, Pruning, Efficient network design and Neural Architecture Search. In this project we will focus on pruning, which seems more promissing, according to the very recent papers [1, 2, 3]. The goal is to improve the neural network pruning in the context of medical applications. 
#### TimeTable
###### dataset collection : Looking for a medical imaging dataset - 1 week
###### Literture Review: The baseline to improve - 2 weeks
###### designing and building an appropriate network- 3 weeks
###### training and fine-tuning that network- 2 weeks
###### building an application to present the results- 1 week
###### writing the final report- 1 week 
###### preparing the presentation of the work - 1 week

#### References
[1] Frankle, Jonathan, and Michael Carbin. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." arXiv preprint arXiv:1803.03635 (2018).<br/>
[2] Zhou, Hattie, et al. "Deconstructing lottery tickets: Zeros, signs, and the supermask." arXiv preprint arXiv:1905.01067 (2019).<br/>
[3] Liu, Zhuang, et al. "Rethinking the value of network pruning." arXiv preprint arXiv:1810.05270 (2018).<br/>
